# @package _global_

potential_module_class: "HessianPotentialModule"

model:
  # Added for eigenvalue/eigenvector prediction
  do_eigvec_1: false
  do_eigvec_2: false  
  do_eigval_1: false
  do_eigval_2: false

  do_hessian: true
  # expressivity
  hessian_alpha_drop: 0.0
  num_layers_hessian: 1
  num_gaussians_distance_hessian: 600
  # if to also use atom type embedding or just relative distances for edge features
  # in edge_distance 
  use_atom_edge_embedding_hessian: True
  # if the same embedding is used in every transformer block
  share_atom_edge_embedding_hessian: False
  reuse_source_target_embedding_hessian: True
  reinit_edge_degree_embedding_hessian: False
  cutoff_hessian: 100.0
  # graph
  otf_graph: false
  hessian_build_method: "1d"


training:
  # trn_path: "data/sample_100-dft-hess-eigen.lmdb"
  # val_path: "data/sample_100-dft-hess-eigen.lmdb"
  follow_batch: ["diag_ij", "edge_index", "message_idx_ij"]

  hessian_loss_weight: 1.0

  hessian_loss_type: "mae"
  mask_hessian: false

  eigen_loss:
    loss_name: "eigenspectrum"
    loss_type: "eigen" # eigen or wa
    k: null
    alpha: null
    dof_filter_fn: null

  max_train_samples: null
  max_val_samples: null
  drop_last: true # otherwise we run into shape issues

pltrainer:
  # max_epochs: 10000
  # gradient_clip_val: 0.1
  # accumulate_grad_batches: 1
  # limit_train_batches: 1600 # 1600 batches * 64 bz = 102400 samples
  limit_val_batches: 200 # 50844 samples / 64 bz = 794.4375 batches
  # log_every_n_steps: 1