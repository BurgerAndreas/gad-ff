# @package _global_

potential_module_class: "HessianPotentialModule"

model:
  do_hessian: true
  # expressivity
  hessian_alpha_drop: 0.0
  num_layers_hessian: 1
  num_gaussians_distance_hessian: 600
  # if to also use atom type embedding or just relative distances for edge features
  # in edge_distance 
  use_atom_edge_embedding_hessian: True
  # if the same embedding is used in every transformer block
  share_atom_edge_embedding_hessian: False
  reuse_source_target_embedding_hessian: True
  reinit_edge_degree_embedding_hessian: False # add edge degree embedding to node features
  cutoff_hessian: 100.0
  # graph
  otf_graph: false
  hessian_build_method: "1d"

  symmetric_messages: False
  symmetric_edges: False
  hessian_no_attn_weights: False # build Hessian off-diagonal elements from raw messages without attention weights

training:
  follow_batch: ["diag_ij", "edge_index", "message_idx_ij"]

  eigen_loss:
    loss_name: "eigenspectrum"
    loss_type: "eigen" # eigen or wa
    k: null
    alpha: null
    dof_filter_fn: null
    dist: "frosq"

  drop_last: true # otherwise we run into shape issues

  # already preprocessed dataset, don't need to do hessian graph transform
  do_hessiangraphtransform: false
