# Eigen training configuration
defaults:
  - ./equiformer_v2
  - _self_
  - lr: step10
  - trgt: eigen
  - preset: blank
  - experiment: base
  - gpu: single

# Model configuration
model_type: "EquiformerV2"
version: "0.1"
project: "gadff"

# Optimizer configuration
optimizer:
  lr: 5e-4  # paper says 3e-4, code uses 5e-4
  betas: [0.9, 0.999]
  weight_decay: 0
  amsgrad: true

# Training configuration  
training:
  trn_path: "data/sample_100-dft-hess-eigen.lmdb"
  val_path: "data/sample_100-dft-hess-eigen.lmdb"
  bz: 128
  bz_val: 128
  num_workers: 2 # 48
  clip_grad: true
  use_clip_gradnorm_queue: true
  gradient_clip_val: 0.1
  # EMA configuration using PyTorch's AveragedModel
  ema:
    enabled: false
    decay: 0.999               # EMA decay factor (higher = slower EMA updates)
    validate_with_ema: true    # Use EMA weights during validation
    save_ema_state: true       # Save EMA state in checkpoints
    use_buffers: true          # Apply EMA to model buffers (BatchNorm stats, etc.)

  train_heads_only: true

  # Checkpoint loading options
  load_weights_only: false  # If true, only load model weights, ignore training state (epoch, optimizer, etc.)

  follow_batch: null
  max_train_samples: null
  max_val_samples: null
  drop_last: false

  do_hessiangraphtransform: true

# Pytorch Lightning Trainer configuration
pltrainer:
  max_epochs: 10000
  gradient_clip_val: 0.1
  accumulate_grad_batches: 1
  limit_train_batches: 1600
  limit_val_batches: 80 # 50844 samples / 64 bz = 794.4375 batches
  log_every_n_steps: 1
  check_val_every_n_epoch: 4  # Run validation every N epochs (default: 1)
  # val_check_interval: 0.5    # Alternative: Run validation every N steps or fraction of epoch
  # overfit_batches: 2

early_stopping:
  monitor: "val-totloss"
  patience: 1000
  mode: "min"
  verbose: true

use_wandb: true

ckpt_model_path: horm

# Checkpoint that contains model weights and optimizerstate , lr state, etc. 
ckpt_trainer_path: null

ckpt_resume_auto: true

ckpt_do_save: true

slurm_job_id: null

# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}
# Changes the current working directory to the output directory for each job
# hydra.job.chdir: False