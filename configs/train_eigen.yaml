# Eigen training configuration
defaults:
  - ./equiformer_v2
  - _self_
  - experiment: base
  - gpu: single

# Model configuration
model_type: "EquiformerV2"
version: "0.1"
project: "gadff"

# Optimizer configuration
optimizer:
  lr: 5e-4  # paper says 3e-4, code uses 5e-4
  betas: [0.9, 0.999]
  weight_decay: 0
  amsgrad: true

# Training configuration  
training:
  trn_path: "data/sample_100-dft-hess-eigen.lmdb"
  val_path: "data/sample_100-dft-hess-eigen.lmdb"
  bz: 128
  num_workers: 16 # 48
  clip_grad: true
  gradient_clip_val: 0.1
  ema: false
  lr_schedule_type: "step"
  lr_schedule_config:
    gamma: 0.85
    step_size: 50

  # Loss weights
  weight_eigval1: 1.0
  weight_eigval2: 1.0
  weight_eigvec1: 1.0
  weight_eigvec2: 1.0

  # Loss type
  loss_type: "mse"
  # Sign-invariant loss for eigenvectors: "cosine", "min_l2", "min_l1", "l1", "l2"
  loss_type_vec: "cosine"  
  
  # Checkpoint loading options
  load_weights_only: false  # If true, only load model weights, ignore training state (epoch, optimizer, etc.)

# Pytorch Lightning Trainer configuration
pltrainer:
  max_epochs: 10000
  gradient_clip_val: 0.1
  accumulate_grad_batches: 1
  limit_train_batches: 1600
  limit_val_batches: 80 
  log_every_n_steps: 10

use_wandb: true

ckpt_model_path: horm

# Checkpoint that contains model weights and optimizerstate , lr state, etc. 
ckpt_trainer_path: null

slurm_job_id: null

# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}
# Changes the current working directory to the output directory for each job
# hydra.job.chdir: False