# @package _global_

# Fit on the smaller training dataset (RGD1), test on TS1x

training:
  trn_path: "RGD1-dft-hess-eigen.lmdb"
  val_path: "ts1x-val-dft-hess-eigen.lmdb"
  bz: 128
  num_workers: 16 # 48
  # lr_schedule_type: "constant"  # No learning rate decay for overfitting
  
pltrainer:
  devices: 4  # Use all 4 GPUs on the node
  num_nodes: 1
  accelerator: "gpu"
  # auto, ddp, ddp_find_unused_parameters_true
  strategy: "ddp"
  max_epochs: 1000  # Fewer epochs since we're overfitting small data
  limit_train_batches: null  # Remove limit to see full tiny dataset 
  limit_val_batches: null    # Remove validation limit
  val_check_interval: 100    # Validate less frequently
  # log_every_n_steps: 10      # Log more frequently to monitor overfitting

experiment_name: "rgd1"