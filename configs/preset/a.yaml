
# @package _global_

model:
  alpha_drop:               0.0         # [0.0, 0.1]
  drop_path_rate:           0.0         # [0.0, 0.05]
  proj_drop:                0.0

# Step decay learning rate schedule
training:
  bz: 128
  bz_val: 128
  
  lr_schedule_type: "step"
  lr_schedule_config:
    gamma: 0.85
    # 20 steps: 5e-4 -> 1e-5
    # over 100 epochs -> step=5
    step_size: 10 # 5 or 10
  
  # hessian_loss_type: "mae"
  mask_hessian: false
  eigen_loss:
    loss_name: "eigenspectrum"
    loss_type: "wa" # eigen or wa
    k: null
    alpha: 1.0
    dof_filter_fn: null
  
pltrainer:
  check_val_every_n_epoch: 4  # Run validation every N epochs (default: 1)
